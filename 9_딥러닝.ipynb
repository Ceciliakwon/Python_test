{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                   [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 측정\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.35871166]\n",
      " [0.89306784]\n",
      " [0.90640277]\n",
      " [0.99313146]\n",
      " [0.9274967 ]\n",
      " [0.99550503]\n",
      " [0.9947917 ]\n",
      " [0.9996977 ]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a= sess.run([train, hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.00846758]\n",
      " [0.04326084]\n",
      " [0.04119492]\n",
      " [0.18532908]\n",
      " [0.0392175 ]\n",
      " [0.17037186]\n",
      " [0.1777153 ]\n",
      " [0.52091974]]\n",
      "예측 : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                   [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a= sess.run([train, hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XOR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.72432184]\n",
      " [0.73946536]\n",
      " [0.7404175 ]\n",
      " [0.7549759 ]\n",
      " [0.7385555 ]\n",
      " [0.7541021 ]\n",
      " [0.7531835 ]\n",
      " [0.7681334 ]]\n",
      "예측 : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                   [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a= sess.run([train, hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]]\n",
    "y_data = [0, 1, 1, 1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련\n",
    "clf = svm.SVC(C=100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 데이터\n",
    "examples = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [1, 1, 0]]\n",
    "examples_label = [0, 0, 1, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(examples_label, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.04727468]\n",
      " [0.95229065]\n",
      " [0.9776745 ]\n",
      " [0.98927104]\n",
      " [0.9883186 ]\n",
      " [0.97556454]\n",
      " [0.95831656]\n",
      " [0.10546982]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], \n",
    "                   [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "### 첫번째 레이어 ###\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "#####################\n",
    "\n",
    "### 두번째 레이어 ###\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "#####################\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a= sess.run([train, hypot2, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[9.6607208e-04]\n",
      " [9.9982822e-01]\n",
      " [9.9955058e-01]\n",
      " [9.9968398e-01]\n",
      " [9.9955606e-01]\n",
      " [9.9935293e-01]\n",
      " [9.9973345e-01]\n",
      " [1.5567243e-03]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 레이어의 수(Deep)는 7개, 입출력 연결의 수(Wide)는  50개로 구현\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], \n",
    "                   [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "### 첫번째 레이어 ###\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "#####################\n",
    "\n",
    "### 두번째 레이어 ###\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "#####################\n",
    "\n",
    "### 세번째 레이어 ###\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias3\")\n",
    "hypot3 = tf.sigmoid(tf.matmul(hypot2, W3) + b3)\n",
    "#####################\n",
    "\n",
    "### 네번째 레이어 ###\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias4\")\n",
    "hypot4 = tf.sigmoid(tf.matmul(hypot3, W4) + b4)\n",
    "#####################\n",
    "\n",
    "### 다섯번째 레이어 ###\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight5\")\n",
    "b5 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias5\")\n",
    "hypot5 = tf.sigmoid(tf.matmul(hypot4, W5) + b5)\n",
    "#####################\n",
    "\n",
    "### 여섯번째 레이어 ###\n",
    "W6 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight6\")\n",
    "b6 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias6\")\n",
    "hypot6 = tf.sigmoid(tf.matmul(hypot5, W6) + b6)\n",
    "#####################\n",
    "\n",
    "### 일곱번째 레이어 ###\n",
    "W7 = tf.Variable(tf.random_normal([50, 1]), tf.float32, name=\"weight7\")\n",
    "b7 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias7\")\n",
    "hypot7 = tf.sigmoid(tf.matmul(hypot6, W7) + b7)\n",
    "#####################\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot7) + (1 - y) * tf.log(1 - hypot7))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot7 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a= sess.run([train, hypot7, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.03805956]\n",
      " [0.9845326 ]\n",
      " [0.98223764]\n",
      " [0.9698165 ]\n",
      " [0.9927269 ]\n",
      " [0.97405016]\n",
      " [0.98060477]\n",
      " [0.07792598]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 축적X, 리셋\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], \n",
    "                   [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "### 첫번째 레이어 ###\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "#####################\n",
    "\n",
    "### 두번째 레이어 ###\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "#####################\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                                       feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2/alpha01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.7498759 ]\n",
      " [0.73500824]\n",
      " [0.7517607 ]\n",
      " [0.7076477 ]\n",
      " [0.77112323]\n",
      " [0.7767981 ]\n",
      " [0.76067775]\n",
      " [0.75336313]]\n",
      "예측 : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.01\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], \n",
    "                   [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "### 첫번째 레이어 ###\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "#####################\n",
    "\n",
    "### 두번째 레이어 ###\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "#####################\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha001\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                                       feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU : Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-84fb6f4620c0>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\cecil\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\cecil\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\cecil\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\cecil\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\cecil\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000001C9259B7BC8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000001C92A28D388>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000001C92C15FEC8>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 모델 구축 : 성능 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# 가설 설정\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.9928344394943922\n",
      "epoch: 2     cost: 0.8883249060674144\n",
      "epoch: 3     cost: 0.7345722663402565\n",
      "epoch: 4     cost: 0.6554171673818069\n",
      "epoch: 5     cost: 0.6003867917711085\n",
      "epoch: 6     cost: 0.5554604703729802\n",
      "epoch: 7     cost: 0.5386538362503048\n",
      "epoch: 8     cost: 0.5071916471828112\n",
      "epoch: 9     cost: 0.4844775360822679\n",
      "epoch: 10     cost: 0.4833734813603486\n",
      "epoch: 11     cost: 0.4597000707821413\n",
      "epoch: 12     cost: 0.43811727003617734\n",
      "epoch: 13     cost: 0.4415623994307085\n",
      "epoch: 14     cost: 0.4164273101091385\n",
      "epoch: 15     cost: 0.422379884177988\n",
      "epoch: 16     cost: 0.4117737538706172\n",
      "epoch: 17     cost: 0.40099754067984517\n",
      "epoch: 18     cost: 0.3979560978846117\n",
      "epoch: 19     cost: 0.3858381902087821\n",
      "epoch: 20     cost: 0.38727109844034385\n",
      "epoch: 21     cost: 0.37991850993849974\n",
      "epoch: 22     cost: 0.37301202324303706\n",
      "epoch: 23     cost: 0.37231593180786465\n",
      "epoch: 24     cost: 0.36206906985152826\n",
      "epoch: 25     cost: 0.3592793646183882\n",
      "epoch: 26     cost: 0.3603260616280817\n",
      "epoch: 27     cost: 0.3585420491478659\n",
      "epoch: 28     cost: 0.3439657376029276\n",
      "epoch: 29     cost: 0.3483708111806349\n",
      "epoch: 30     cost: 0.34452892606908614\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "    \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9103\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어 3개 추가, 입출력 갯수는 256개 : Relu 사용 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 4.268665814833207\n",
      "epoch: 2     cost: 3.1716627055948456\n",
      "epoch: 3     cost: 2.0226868755167176\n",
      "epoch: 4     cost: 1.6518906909769233\n",
      "epoch: 5     cost: 1.2323645791140474\n",
      "epoch: 6     cost: 1.1931929050792354\n",
      "epoch: 7     cost: 0.9629608410054982\n",
      "epoch: 8     cost: 0.8929230703007095\n",
      "epoch: 9     cost: 0.811810335245999\n",
      "epoch: 10     cost: 0.7505769354646858\n",
      "epoch: 11     cost: 0.6999862582033333\n",
      "epoch: 12     cost: 0.702906021313234\n",
      "epoch: 13     cost: 0.6762301180579443\n",
      "epoch: 14     cost: 0.6549518609046936\n",
      "epoch: 15     cost: 0.5770335829257964\n",
      "epoch: 16     cost: 0.5721774001555008\n",
      "epoch: 17     cost: 0.5484042876416984\n",
      "epoch: 18     cost: 0.5309830011021008\n",
      "epoch: 19     cost: 0.5370326730338008\n",
      "epoch: 20     cost: 0.5157156626744703\n",
      "epoch: 21     cost: 0.5030097834630446\n",
      "epoch: 22     cost: 0.4711532923308287\n",
      "epoch: 23     cost: 0.4720823467861522\n",
      "epoch: 24     cost: 0.4652780277078802\n",
      "epoch: 25     cost: 0.4484258402477614\n",
      "epoch: 26     cost: 0.42300332383675987\n",
      "epoch: 27     cost: 0.4356803574345328\n",
      "epoch: 28     cost: 0.430789877447215\n",
      "epoch: 29     cost: 0.4691292346607557\n",
      "epoch: 30     cost: 0.42536289697343654\n",
      "훈련 종료\n",
      "정확도 :  0.8872\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "### 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "### 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "### 세번째 레이어\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "### 네번째 레이어\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "    \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier 초기화 : 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.7575447385419498\n",
      "epoch: 2     cost: 0.2554127418453045\n",
      "epoch: 3     cost: 0.18819360684264794\n",
      "epoch: 4     cost: 0.14396489106796\n",
      "epoch: 5     cost: 0.11888473288579421\n",
      "epoch: 6     cost: 0.09855760608207098\n",
      "epoch: 7     cost: 0.08539548539302556\n",
      "epoch: 8     cost: 0.07144889292391861\n",
      "epoch: 9     cost: 0.06246838264167307\n",
      "epoch: 10     cost: 0.05279427509077567\n",
      "epoch: 11     cost: 0.046531743468208715\n",
      "epoch: 12     cost: 0.039864226989448026\n",
      "epoch: 13     cost: 0.03458569161932576\n",
      "epoch: 14     cost: 0.02996500868350267\n",
      "epoch: 15     cost: 0.026685321818698534\n",
      "epoch: 16     cost: 0.022463584299283947\n",
      "epoch: 17     cost: 0.019384981155056838\n",
      "epoch: 18     cost: 0.01630219841375947\n",
      "epoch: 19     cost: 0.013850770940665492\n",
      "epoch: 20     cost: 0.012144268992441627\n",
      "epoch: 21     cost: 0.010043131339956421\n",
      "epoch: 22     cost: 0.008616524757394058\n",
      "epoch: 23     cost: 0.007694510702382434\n",
      "epoch: 24     cost: 0.006761311286721719\n",
      "epoch: 25     cost: 0.005227211679077963\n",
      "epoch: 26     cost: 0.004910081329501488\n",
      "epoch: 27     cost: 0.004313311046836051\n",
      "epoch: 28     cost: 0.0037501349067315466\n",
      "epoch: 29     cost: 0.00353577543515712\n",
      "epoch: 30     cost: 0.0029990819335745794\n",
      "훈련 종료\n",
      "정확도 :  0.9804\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "### 첫번째 레이어\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "### 두번째 레이어\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "### 세번째 레이어\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "### 네번째 레이어\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "    \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 더 deep하고 wide하게 : layer는 총 8개로 구성, 입출력 갯수는 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.8337972439419141\n",
      "epoch: 2     cost: 0.503221289840612\n",
      "epoch: 3     cost: 0.2743173028122297\n",
      "epoch: 4     cost: 0.2086477190797979\n",
      "epoch: 5     cost: 0.16553207942030634\n",
      "epoch: 6     cost: 0.14221946387128392\n",
      "epoch: 7     cost: 0.12072554687884714\n",
      "epoch: 8     cost: 0.0998698608238589\n",
      "epoch: 9     cost: 0.12684873939915134\n",
      "epoch: 10     cost: 0.0765393363752149\n",
      "epoch: 11     cost: 0.06755171628838234\n",
      "epoch: 12     cost: 0.06086520035158505\n",
      "epoch: 13     cost: 0.05570433971556752\n",
      "epoch: 14     cost: 0.04695080377500164\n",
      "epoch: 15     cost: 0.04414970028468153\n",
      "epoch: 16     cost: 0.8539453022893178\n",
      "epoch: 17     cost: 0.12140077563172036\n",
      "epoch: 18     cost: 0.06784844365986911\n",
      "epoch: 19     cost: 0.05503366590392858\n",
      "epoch: 20     cost: 0.04417025782845234\n",
      "epoch: 21     cost: 0.03405546518720011\n",
      "epoch: 22     cost: 0.03005121893672778\n",
      "epoch: 23     cost: 0.024459104407578702\n",
      "epoch: 24     cost: 0.020239537778649154\n",
      "epoch: 25     cost: 0.017242810428989206\n",
      "epoch: 26     cost: 0.01409505784807896\n",
      "epoch: 27     cost: 0.010163729103620752\n",
      "epoch: 28     cost: 0.00769962582938288\n",
      "epoch: 29     cost: 0.051056549617311583\n",
      "epoch: 30     cost: 0.33023372427645054\n",
      "훈련 종료\n",
      "정확도 :  0.9708\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "### 첫번째 레이어\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "### 두번째 레이어\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "### 세번째 레이어\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "\n",
    "### 네번째 레이어\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "\n",
    "### 다섯번째 레이어\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "\n",
    "### 여섯번째 레이어\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "\n",
    "### 일곱번째 레이어\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "\n",
    "### 여덟번째 레이어\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "    \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop out : 과적합 해결 방안 + AdamOptimizer : 98.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.9649706342003564\n",
      "epoch: 2     cost: 0.24534123483029288\n",
      "epoch: 3     cost: 0.1806283236904578\n",
      "epoch: 4     cost: 0.14547101977196605\n",
      "epoch: 5     cost: 0.12744041143493218\n",
      "epoch: 6     cost: 0.11732397120107305\n",
      "epoch: 7     cost: 0.10372606719082059\n",
      "epoch: 8     cost: 0.0895064096559178\n",
      "epoch: 9     cost: 0.085140387178822\n",
      "epoch: 10     cost: 0.08163625553927635\n",
      "epoch: 11     cost: 0.07155636811120945\n",
      "epoch: 12     cost: 0.06776770272376859\n",
      "epoch: 13     cost: 0.0635335905748335\n",
      "epoch: 14     cost: 0.06232961911200122\n",
      "epoch: 15     cost: 0.06062140134417197\n",
      "epoch: 16     cost: 0.06280819060450249\n",
      "epoch: 17     cost: 0.056015778727490764\n",
      "epoch: 18     cost: 0.054283463687381954\n",
      "epoch: 19     cost: 0.05562033459205522\n",
      "epoch: 20     cost: 0.05366593664766035\n",
      "epoch: 21     cost: 0.056362474898553734\n",
      "epoch: 22     cost: 0.04827705300159077\n",
      "epoch: 23     cost: 0.049454378311691645\n",
      "epoch: 24     cost: 0.04668156076942318\n",
      "epoch: 25     cost: 0.046758047107776514\n",
      "epoch: 26     cost: 0.046293778293342705\n",
      "epoch: 27     cost: 0.043388422967188735\n",
      "epoch: 28     cost: 0.043441370574080124\n",
      "epoch: 29     cost: 0.03825214304707269\n",
      "epoch: 30     cost: 0.042938432940705235\n",
      "훈련 종료\n",
      "정확도 :  0.9815\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "### 첫번째 레이어\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
    "\n",
    "### 두번째 레이어\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
    "\n",
    "### 세번째 레이어\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "hypot3 = tf.nn.dropout(hypot3, keep_prob=prob)\n",
    "\n",
    "### 네번째 레이어\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "hypot4 = tf.nn.dropout(hypot4, keep_prob=prob)\n",
    "\n",
    "### 다섯번째 레이어\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "hypot5 = tf.nn.dropout(hypot5, keep_prob=prob)\n",
    "\n",
    "### 여섯번째 레이어\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "hypot6 = tf.nn.dropout(hypot6, keep_prob=prob)\n",
    "\n",
    "### 일곱번째 레이어\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "hypot7 = tf.nn.dropout(hypot7, keep_prob=prob)\n",
    "\n",
    "### 여덟번째 레이어\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "hypot8 = tf.nn.dropout(hypot8, keep_prob=prob)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "    \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
